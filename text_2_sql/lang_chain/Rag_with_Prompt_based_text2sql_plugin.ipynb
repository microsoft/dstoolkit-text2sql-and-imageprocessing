{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import yaml\n",
    "import dotenv\n",
    "import json\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.schema import BaseOutputParser\n",
    "from IPython.display import display, Markdown\n",
    "from plugins.prompt_based_sql_plugin.prompt_based_sql_plugin import PromptBasedSQLPlugin\n",
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dotenv.load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the OpenAI LLM\n",
    "llm = OpenAI(api_key=os.getenv(\"OpenAI__ApiKey\"))\n",
    "\n",
    "# Initialize the SQL Plugin\n",
    "sql_plugin = PromptBasedSQLPlugin(database=os.getenv(\"Text2Sql__DatabaseName\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load prompt and execution settings from the file\n",
    "with open(\"./prompt.yaml\", \"r\") as file:\n",
    "    data = yaml.safe_load(file.read())\n",
    "\n",
    "# Create a prompt template\n",
    "prompt_template = PromptTemplate(\n",
    "    input_variables=[\"chat_history\", \"important_information\", \"user_input\"],\n",
    "    template=data[\"template\"]\n",
    ")\n",
    "\n",
    "# Create an LLMChain\n",
    "chain = LLMChain(llm=llm, prompt=prompt_template)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize chat history\n",
    "history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def ask_question(question: str, chat_history: list) -> str:\n",
    "    \"\"\"Asks a question to the chatbot and returns the answer.\n",
    "    \n",
    "    Args:\n",
    "        question (str): The question to ask the chatbot.\n",
    "        chat_history (list): The chat history list.\n",
    "        \n",
    "    Returns:\n",
    "        str: The answer from the chatbot.\n",
    "    \"\"\"\n",
    "\n",
    "    # Create important information prompt that contains the SQL database information.\n",
    "    engine_specific_rules = \"Use TOP X to limit the number of rows returned instead of LIMIT X. NEVER USE LIMIT X as it produces a syntax error.\"\n",
    "    important_information_prompt = f\"\"\"\n",
    "    [SQL DATABASE INFORMATION]\n",
    "    {sql_plugin.system_prompt(engine_specific_rules=engine_specific_rules)}\n",
    "    [END SQL DATABASE INFORMATION]\n",
    "    \"\"\"\n",
    "\n",
    "    # Prepare the input for the LLMChain\n",
    "    inputs = {\n",
    "        \"chat_history\": chat_history,\n",
    "        \"important_information\": important_information_prompt,\n",
    "        \"user_input\": question\n",
    "    }\n",
    "\n",
    "    logging.info(\"Question: %s\", question)\n",
    "\n",
    "    # Invoke the LLMChain\n",
    "    answer = await chain(inputs)\n",
    "\n",
    "    logging.info(\"Answer: %s\", answer)\n",
    "\n",
    "    # Log the question and answer to the chat history.\n",
    "    chat_history.append({\"role\": \"user\", \"message\": question})\n",
    "    chat_history.append({\"role\": \"assistant\", \"message\": answer})\n",
    "\n",
    "    json_answer = json.loads(str(answer))\n",
    "\n",
    "    display(Markdown(json_answer[\"answer\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "await ask_question(\"What are the different product categories we have?\", history)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
