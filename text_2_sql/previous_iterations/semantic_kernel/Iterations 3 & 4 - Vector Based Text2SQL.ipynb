{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Copyright (c) Microsoft Corporation.\n",
        "\n",
        "Licensed under the MIT License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Text2SQL with Semantic Kernel & Azure OpenAI\n",
        "\n",
        "This notebook demonstrates how the SQL plugin can be integrated with Semantic Kernel and Azure OpenAI to answer questions from the database based on the schemas provided. \n",
        "\n",
        "A multi-shot approach is used for SQL generation for more reliable results and reduced token usage. More details can be found in the README.md.\n",
        "\n",
        "### Dependencies\n",
        "\n",
        "To install dependencies:\n",
        "\n",
        "`uv sync --package semantic_kernel_text_2_sql`\n",
        "\n",
        "`uv add --editable text_2_sql_core`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# This is only needed for this notebook to work\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "# Add the parent directory of `src` to the path\n",
        "sys.path.append(str(Path.cwd() / \"src\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1718623217703
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "import logging\n",
        "import os\n",
        "import yaml\n",
        "import dotenv\n",
        "import json\n",
        "from semantic_kernel.connectors.ai.open_ai import (\n",
        "    AzureChatCompletion,\n",
        ")\n",
        "from semantic_kernel.contents.chat_history import ChatHistory\n",
        "from semantic_kernel.kernel import Kernel\n",
        "from semantic_kernel_text_2_sql.plugins.vector_based_sql_plugin.vector_based_sql_plugin import VectorBasedSQLPlugin\n",
        "from semantic_kernel.functions.kernel_arguments import KernelArguments\n",
        "from semantic_kernel.prompt_template.prompt_template_config import PromptTemplateConfig\n",
        "from IPython.display import display, Markdown\n",
        "logging.basicConfig(level=logging.INFO)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Kernel Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dotenv.load_dotenv()\n",
        "kernel = Kernel()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Set up GPT connections"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1718623218006
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "service_id = \"chat\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1718623218267
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "chat_service = AzureChatCompletion(\n",
        "    service_id=service_id,\n",
        "    deployment_name=os.environ[\"OpenAI__CompletionDeployment\"],\n",
        "    endpoint=os.environ[\"OpenAI__Endpoint\"],\n",
        "    api_key=os.environ[\"OpenAI__ApiKey\"],\n",
        ")\n",
        "kernel.add_service(chat_service)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1718623218614
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Register the SQL Plugin with the Database name to use.\n",
        "sql_plugin = VectorBasedSQLPlugin()\n",
        "kernel.add_plugin(sql_plugin, \"SQL\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Prompt Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load prompt and execution settings from the file\n",
        "with open(\"./semantic_kernel_text_2_sql/src/prompt.yaml\", \"r\") as file:\n",
        "    data = yaml.safe_load(file.read())\n",
        "    prompt_template_config = PromptTemplateConfig(**data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "chat_function = kernel.add_function(\n",
        "    prompt_template_config=prompt_template_config,\n",
        "    plugin_name=\"ChatBot\",\n",
        "    function_name=\"Chat\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ChatBot setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "history = ChatHistory()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "async def ask_question(question: str, chat_history: ChatHistory) -> str:\n",
        "    \"\"\"Asks a question to the chatbot and returns the answer.\n",
        "    \n",
        "    Args:\n",
        "        question (str): The question to ask the chatbot.\n",
        "        chat_history (ChatHistory): The chat history object.\n",
        "        \n",
        "    Returns:\n",
        "        str: The answer from the chatbot.\n",
        "    \"\"\"\n",
        "\n",
        "    # Create important information prompt that contains the SQL database information.\n",
        "    engine_specific_rules = \"Use TOP X to limit the number of rows returned instead of LIMIT X. NEVER USE LIMIT X as it produces a syntax error.\"\n",
        "    sql_database_information_prompt = f\"\"\"\n",
        "    [SQL DATABASE INFORMATION]\n",
        "    {await sql_plugin.sql_prompt_injection(engine_specific_rules=engine_specific_rules, question=question)}\n",
        "    [END SQL DATABASE INFORMATION]\n",
        "    \"\"\"\n",
        "\n",
        "    arguments = KernelArguments()\n",
        "    arguments[\"sql_database_information\"] = sql_database_information_prompt\n",
        "    arguments[\"user_input\"] = question\n",
        "\n",
        "    logging.info(\"Question: %s\", question)\n",
        "\n",
        "    answer = await kernel.invoke(\n",
        "        function_name=\"Chat\",\n",
        "        plugin_name=\"ChatBot\",\n",
        "        arguments=arguments,\n",
        "        chat_history=chat_history,\n",
        "    )\n",
        "\n",
        "    logging.info(\"Answer: %s\", answer)\n",
        "\n",
        "    # Log the question and answer to the chat history.\n",
        "    chat_history.add_user_message(question)\n",
        "    chat_history.add_message({\"role\": \"assistant\", \"message\": answer})\n",
        "\n",
        "    json_answer = json.loads(str(answer))\n",
        "\n",
        "    display(Markdown(json_answer[\"answer\"]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "await ask_question(\"What are the different product categories we have?\", history)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "await ask_question(\"What is the top performing product by quantity of units sold?\", history)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "await ask_question(\"Which country did we sell the most to in June 2008?\", history)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "python310-sdkv2"
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.6"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      },
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
