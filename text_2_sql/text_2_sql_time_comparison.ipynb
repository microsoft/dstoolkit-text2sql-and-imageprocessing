{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Copyright (c) Microsoft Corporation.\n",
        "\n",
        "Licensed under the MIT License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Time Comparison For Approaches\n",
        "\n",
        "This notebook demonstrates how the SQL plugin can be integrated with Semantic Kernel and Azure OpenAI to answer questions from the database based on the schemas provided. \n",
        "\n",
        "A multi-shot approach is used for SQL generation for more reliable results and reduced token usage. More details can be found in the README.md."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "gather": {
          "logged": 1718623217703
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "import logging\n",
        "import os\n",
        "import yaml\n",
        "import dotenv\n",
        "import json\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from semantic_kernel.connectors.ai.open_ai import (\n",
        "    AzureChatCompletion,\n",
        ")\n",
        "from semantic_kernel.contents.chat_history import ChatHistory\n",
        "from semantic_kernel.kernel import Kernel\n",
        "from plugins.vector_based_sql_plugin.vector_based_sql_plugin import VectorBasedSQLPlugin\n",
        "from semantic_kernel.functions.kernel_arguments import KernelArguments\n",
        "from semantic_kernel.prompt_template.prompt_template_config import PromptTemplateConfig\n",
        "from utils.sql import fetch_queries_from_cache, add_queries_to_cache\n",
        "logging.basicConfig(level=logging.INFO)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Kernel Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "dotenv.load_dotenv()\n",
        "kernel = Kernel()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Set up GPT connections"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "gather": {
          "logged": 1718623218006
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "service_id = \"chat\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "gather": {
          "logged": 1718623218267
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "chat_service = AzureChatCompletion(\n",
        "    service_id=service_id,\n",
        "    deployment_name=os.environ[\"OpenAI__CompletionDeployment\"],\n",
        "    endpoint=os.environ[\"OpenAI__Endpoint\"],\n",
        "    api_key=os.environ[\"OpenAI__ApiKey\"],\n",
        ")\n",
        "kernel.add_service(chat_service)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "gather": {
          "logged": 1718623218614
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "KernelPlugin(name='SQL', description=None, functions={'GetEntitySchema': KernelFunctionFromMethod(metadata=KernelFunctionMetadata(name='GetEntitySchema', plugin_name='SQL', description='Gets the schema of a view or table in the SQL Database by selecting the most relevant entity based on the search term. Extract key terms from the user question and use these as the search term. Several entities may be returned.', parameters=[KernelParameterMetadata(name='text', description='The text to run a semantic search against. Relevant entities will be returned.', default_value=None, type_='str', is_required=True, type_object=<class 'str'>, schema_data={'type': 'string', 'description': 'The text to run a semantic search against. Relevant entities will be returned.'}, function_schema_include=True)], is_prompt=False, is_asynchronous=True, return_parameter=KernelParameterMetadata(name='return', description='', default_value=None, type_='str', is_required=True, type_object=<class 'str'>, schema_data={'type': 'string'}, function_schema_include=True), additional_properties={}), invocation_duration_histogram=<opentelemetry.metrics._internal.instrument._ProxyHistogram object at 0x7f37125f4500>, streaming_duration_histogram=<opentelemetry.metrics._internal.instrument._ProxyHistogram object at 0x7f37135ddc70>, method=<bound method VectorBasedSQLPlugin.get_entity_schemas of <plugins.vector_based_sql_plugin.vector_based_sql_plugin.VectorBasedSQLPlugin object at 0x7f36eed0d760>>, stream_method=None), 'RunSQLQuery': KernelFunctionFromMethod(metadata=KernelFunctionMetadata(name='RunSQLQuery', plugin_name='SQL', description='Runs an SQL query against the SQL Database to extract information.', parameters=[KernelParameterMetadata(name='sql_query', description='The SQL query to run against the DB', default_value=None, type_='str', is_required=True, type_object=<class 'str'>, schema_data={'type': 'string', 'description': 'The SQL query to run against the DB'}, function_schema_include=True)], is_prompt=False, is_asynchronous=True, return_parameter=KernelParameterMetadata(name='return', description='', default_value=None, type_='str', is_required=True, type_object=<class 'str'>, schema_data={'type': 'string'}, function_schema_include=True), additional_properties={}), invocation_duration_histogram=<opentelemetry.metrics._internal.instrument._ProxyHistogram object at 0x7f37125f4500>, streaming_duration_histogram=<opentelemetry.metrics._internal.instrument._ProxyHistogram object at 0x7f37135ddc70>, method=<bound method VectorBasedSQLPlugin.run_sql_query of <plugins.vector_based_sql_plugin.vector_based_sql_plugin.VectorBasedSQLPlugin object at 0x7f36eed0d760>>, stream_method=None)})"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Register the SQL Plugin with the Database name to use.\n",
        "sql_plugin = VectorBasedSQLPlugin(database=os.environ[\"Text2Sql__DatabaseName\"])\n",
        "kernel.add_plugin(sql_plugin, \"SQL\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Prompt Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load prompt and execution settings from the file\n",
        "with open(\"./query_cache_based_prompt.yaml\", \"r\") as file:\n",
        "    data = yaml.safe_load(file.read())\n",
        "    prompt_template_config = PromptTemplateConfig(**data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "chat_function = kernel.add_function(\n",
        "    prompt_template_config=prompt_template_config,\n",
        "    plugin_name=\"ChatBot\",\n",
        "    function_name=\"Chat\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ChatBot setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "async def ask_question(question: str, chat_history: ChatHistory) -> str:\n",
        "    \"\"\"Asks a question to the chatbot and returns the answer.\n",
        "    \n",
        "    Args:\n",
        "        question (str): The question to ask the chatbot.\n",
        "        chat_history (ChatHistory): The chat history object.\n",
        "        \n",
        "    Returns:\n",
        "        str: The answer from the chatbot.\n",
        "    \"\"\"\n",
        "\n",
        "    formatted_sql_cache_string = await fetch_queries_from_cache(question)\n",
        "\n",
        "    # Create important information prompt that contains the SQL database information.\n",
        "    engine_specific_rules = \"Use TOP X to limit the number of rows returned instead of LIMIT X. NEVER USE LIMIT X as it produces a syntax error.\"\n",
        "    important_information_prompt = f\"\"\"\n",
        "    [SQL DATABASE INFORMATION]\n",
        "    {sql_plugin.system_prompt(engine_specific_rules=engine_specific_rules)}\n",
        "    [END SQL DATABASE INFORMATION]\n",
        "    \"\"\"\n",
        "\n",
        "    question_string = f\"{question}\\n{formatted_sql_cache_string}\"\n",
        "\n",
        "    arguments = KernelArguments()\n",
        "    arguments[\"chat_history\"] = chat_history\n",
        "    arguments[\"important_information\"] = important_information_prompt\n",
        "    arguments[\"user_input\"] = question_string\n",
        "\n",
        "    logging.info(\"Question: %s\", question)\n",
        "\n",
        "    answer = await kernel.invoke(\n",
        "        function_name=\"Chat\",\n",
        "        plugin_name=\"ChatBot\",\n",
        "        arguments=arguments,\n",
        "        chat_history=chat_history,\n",
        "    )\n",
        "\n",
        "    logging.info(\"Answer: %s\", answer)\n",
        "\n",
        "    json_answer = json.loads(str(answer))\n",
        "\n",
        "    await add_queries_to_cache(question, json_answer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "async def measure_average_time(question: str, n=10) -> float:\n",
        "    total_time = 0.0\n",
        "    history = ChatHistory()\n",
        "    \n",
        "    for _ in range(n):\n",
        "        start_time = time.time()\n",
        "        await ask_question(question, history)\n",
        "        total_time += (time.time() - start_time)\n",
        "        \n",
        "    # Return the average time taken\n",
        "    return total_time / n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "async def run_tests():\n",
        "    scenarios = ['Vector', 'QueryCache', 'PreRunQueryCache']\n",
        "\n",
        "    # Define your six questions\n",
        "    questions = [\n",
        "        \"Give me the product details, quantity, and price for order number 12345.\",\n",
        "        \"Tell me the shipping addresses for all sales orders placed in 2023.\",\n",
        "        \"Give me a list of all products, including their descriptions and categories.\",\n",
        "        \"Tell me all sales orders that include the product 'Mountain Bike', along with the order date and customer address.\",\n",
        "        \"Give me a list of all products and their model catalog descriptions.\",\n",
        "        \"Tell me the total sales amounts for each product category.\"\n",
        "    ]\n",
        "\n",
        "    # Store average times for each question and scenario\n",
        "    average_times = {scenario: [] for scenario in scenarios}\n",
        "\n",
        "    # Run each scenario and measure times\n",
        "    for scenario in scenarios:\n",
        "        if scenario == \"Vector\":\n",
        "            os.environ[\"Text2Sql__UseCache\"] = \"False\"\n",
        "            os.environ[\"Text2Sql__PreRunQueryCache\"] = \"False\"\n",
        "        elif scenario == \"QueryCache\":\n",
        "            os.environ[\"Text2Sql__UseCache\"] = \"True\"\n",
        "            os.environ[\"Text2Sql__PreRunQueryCache\"] = \"False\"\n",
        "        elif scenario == \"PreRunQueryCache\":\n",
        "            os.environ[\"Text2Sql__UseCache\"] = \"True\"\n",
        "            os.environ[\"Text2Sql__PreRunQueryCache\"] = \"True\"\n",
        "            \n",
        "        for question in questions:\n",
        "            avg_time = await measure_average_time(question)\n",
        "            average_times[scenario].append(avg_time)\n",
        "\n",
        "    return average_times\n",
        "\n",
        "# Run the tests\n",
        "average_times = await run_tests()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_average_times(average_times):\n",
        "    # Set width of bars\n",
        "    bar_width = 0.25\n",
        "\n",
        "    # Set position of bars on x-axis\n",
        "    r1 = np.arange(6)\n",
        "    r2 = [x + bar_width for x in r1]\n",
        "    r3 = [x + bar_width for x in r2]\n",
        "\n",
        "    # Make the plot\n",
        "    plt.bar(r1, average_times['Vector'], color='b', width=bar_width, edgecolor='grey', label='Vector')\n",
        "    plt.bar(r2, average_times['QueryCache'], color='g', width=bar_width, edgecolor='grey', label='QueryCache')\n",
        "    plt.bar(r3, average_times['PreRunQueryCache'], color='r', width=bar_width, edgecolor='grey', label='PreRunQueryCache')\n",
        "\n",
        "    # Add labels and title\n",
        "    plt.xlabel('Questions', fontweight='bold')\n",
        "    plt.ylabel('Average Time (seconds)', fontweight='bold')\n",
        "    plt.title('Average Response Time per Scenario')\n",
        "\n",
        "    # Add xticks on the middle of the group bars\n",
        "    plt.xticks([r + bar_width for r in range(6)], ['Q1', 'Q2', 'Q3', 'Q4', 'Q5', 'Q6'])\n",
        "\n",
        "    # Create legend & show graphic\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "# Plot the average times\n",
        "plot_average_times(average_times)\n"
      ]
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "python310-sdkv2"
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.6"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      },
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
