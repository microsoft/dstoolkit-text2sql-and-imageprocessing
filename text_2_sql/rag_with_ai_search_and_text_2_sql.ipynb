{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Copyright (c) Microsoft Corporation.\n",
        "\n",
        "Licensed under the MIT License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Text2SQL & AI Search with Semantic Kernel & Azure OpenAI\n",
        "\n",
        "This notebook demonstrates how the SQL plugin can be integrated with Semantic Kernel and Azure OpenAI to answer questions from the database based on the schemas provided. Additionally, it integrates with an AI Search plugin to show how both can be used in parallel to answer questions based on the best possible source. The prompt may need to be tweaked for your usage.\n",
        "\n",
        "A multi-shot approach is used for SQL generation for more reliable results and reduced token usage. More details can be found in the README.md."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "gather": {
          "logged": 1718623217703
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "import logging\n",
        "import os\n",
        "import yaml\n",
        "import dotenv\n",
        "import json\n",
        "from semantic_kernel.connectors.ai.open_ai import (\n",
        "    AzureChatCompletion,\n",
        ")\n",
        "from semantic_kernel.contents.chat_history import ChatHistory\n",
        "from semantic_kernel.kernel import Kernel\n",
        "from plugins.prompt_based_sql_plugin.prompt_based_sql_plugin import PromptBasedSQLPlugin\n",
        "from plugins.ai_search_plugin.ai_search_plugin import AISearchPlugin\n",
        "from semantic_kernel.functions.kernel_arguments import KernelArguments\n",
        "from semantic_kernel.prompt_template.prompt_template_config import PromptTemplateConfig\n",
        "from IPython.display import display, Markdown\n",
        "\n",
        "logging.basicConfig(level=logging.INFO)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Kernel Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "dotenv.load_dotenv()\n",
        "kernel = Kernel()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Set up GPT connections"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "gather": {
          "logged": 1718623218006
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "service_id = \"chat\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "gather": {
          "logged": 1718623218267
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "chat_service = AzureChatCompletion(\n",
        "    service_id=service_id,\n",
        "    deployment_name=os.environ[\"OpenAI__CompletionDeployment\"],\n",
        "    endpoint=os.environ[\"OpenAI__Endpoint\"],\n",
        "    api_key=os.environ[\"OpenAI__ApiKey\"],\n",
        ")\n",
        "kernel.add_service(chat_service)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "gather": {
          "logged": 1718623218614
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "KernelPlugin(name='SQL', description=None, functions={'GetEntitySchema': KernelFunctionFromMethod(metadata=KernelFunctionMetadata(name='GetEntitySchema', plugin_name='SQL', description='Get the detailed schema of an entity in the Database. Use the entity and the column returned to formulate a SQL query. The view name or table name must be one of the ENTITY NAMES defined in the [ENTITIES LIST]. Only use the column names obtained from GetEntitySchema() when constructing a SQL query, do not make up column names.', parameters=[KernelParameterMetadata(name='entity_name', description='The view or table name to get the schema for. It must be one of the ENTITY NAMES defined in the [ENTITIES LIST] function.', default_value=None, type_='str', is_required=True, type_object=<class 'str'>, schema_data={'type': 'string', 'description': 'The view or table name to get the schema for. It must be one of the ENTITY NAMES defined in the [ENTITIES LIST] function.'}, function_schema_include=True)], is_prompt=False, is_asynchronous=True, return_parameter=KernelParameterMetadata(name='return', description='', default_value=None, type_='str', is_required=True, type_object=<class 'str'>, schema_data={'type': 'string'}, function_schema_include=True), additional_properties={}), invocation_duration_histogram=<opentelemetry.metrics._internal.instrument._ProxyHistogram object at 0x7f94ab79e7b0>, streaming_duration_histogram=<opentelemetry.metrics._internal.instrument._ProxyHistogram object at 0x7f94a97e38f0>, method=<bound method SQLPlugin.get_entity_schema of <plugins.sql_plugin.sql_plugin.SQLPlugin object at 0x7f9483c23440>>, stream_method=None), 'RunSQLQuery': KernelFunctionFromMethod(metadata=KernelFunctionMetadata(name='RunSQLQuery', plugin_name='SQL', description='Runs an SQL query against the SQL Database to extract information.', parameters=[KernelParameterMetadata(name='sql_query', description='The SQL query to run against the DB', default_value=None, type_='str', is_required=True, type_object=<class 'str'>, schema_data={'type': 'string', 'description': 'The SQL query to run against the DB'}, function_schema_include=True)], is_prompt=False, is_asynchronous=True, return_parameter=KernelParameterMetadata(name='return', description='', default_value=None, type_='str', is_required=True, type_object=<class 'str'>, schema_data={'type': 'string'}, function_schema_include=True), additional_properties={}), invocation_duration_histogram=<opentelemetry.metrics._internal.instrument._ProxyHistogram object at 0x7f94ab79e7b0>, streaming_duration_histogram=<opentelemetry.metrics._internal.instrument._ProxyHistogram object at 0x7f94a97e38f0>, method=<bound method SQLPlugin.run_sql_query of <plugins.sql_plugin.sql_plugin.SQLPlugin object at 0x7f9483c23440>>, stream_method=None)})"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Register the SQL Plugin with the Database name to use.\n",
        "sql_plugin = PromptBasedSQLPlugin(database=os.environ[\"Text2Sql__DatabaseName\"])\n",
        "kernel.add_plugin(sql_plugin, \"SQL\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "KernelPlugin(name='AISearch', description=None, functions={'QueryDocumentStorage': KernelFunctionFromMethod(metadata=KernelFunctionMetadata(name='QueryDocumentStorage', plugin_name='AISearch', description='Runs an hybrid semantic search against some text to return relevant documents that are indexed within AI Search.', parameters=[KernelParameterMetadata(name='text', description='The text to run a semantic search against.', default_value=None, type_='str', is_required=True, type_object=<class 'str'>, schema_data={'type': 'string', 'description': 'The text to run a semantic search against.'}, function_schema_include=True)], is_prompt=False, is_asynchronous=True, return_parameter=KernelParameterMetadata(name='return', description='', default_value=None, type_='str', is_required=True, type_object=<class 'str'>, schema_data={'type': 'string'}, function_schema_include=True), additional_properties={}), invocation_duration_histogram=<opentelemetry.metrics._internal.instrument._ProxyHistogram object at 0x7f94ab79e7b0>, streaming_duration_histogram=<opentelemetry.metrics._internal.instrument._ProxyHistogram object at 0x7f94a97e38f0>, method=<bound method AISearchPlugin.query_document_storage of <plugins.ai_search_plugin.ai_search_plugin.AISearchPlugin object at 0x7f9483c23f20>>, stream_method=None)})"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ai_search_plugin = AISearchPlugin()\n",
        "kernel.add_plugin(ai_search_plugin, \"AISearch\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Prompt Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load prompt and execution settings from the file\n",
        "with open(\"./prompt_based_prompt.yaml\", \"r\") as file:\n",
        "    data = yaml.safe_load(file.read())\n",
        "    prompt_template_config = PromptTemplateConfig(**data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "chat_function = kernel.add_function(\n",
        "    prompt_template_config=prompt_template_config,\n",
        "    plugin_name=\"ChatBot\",\n",
        "    function_name=\"Chat\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ChatBot setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "history = ChatHistory()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "async def ask_question(question: str, chat_history: ChatHistory) -> str:\n",
        "    \"\"\"Asks a question to the chatbot and returns the answer.\n",
        "    \n",
        "    Args:\n",
        "        question (str): The question to ask the chatbot.\n",
        "        chat_history (ChatHistory): The chat history object.\n",
        "        \n",
        "    Returns:\n",
        "        str: The answer from the chatbot.\n",
        "    \"\"\"\n",
        "\n",
        "    # Create important information prompt that contains the SQL database information and the AI search description (useful if you have multiple indexes).\n",
        "    engine_specific_rules = \"Use TOP X to limit the number of rows returned instead of LIMIT X. NEVER USE LIMIT X as it produces a syntax error.\"\n",
        "    important_information_prompt = f\"\"\"\n",
        "    [AI SEARCH INFORMATION]\n",
        "    {ai_search_plugin.system_prompt()}\n",
        "    [END AI SEARCH INFORMATION]\n",
        "\n",
        "    [SQL DATABASE INFORMATION]\n",
        "    {sql_plugin.system_prompt(engine_specific_rules=engine_specific_rules)}\n",
        "    [END SQL DATABASE INFORMATION]\n",
        "    \"\"\"\n",
        "\n",
        "    arguments = KernelArguments()\n",
        "    arguments[\"chat_history\"] = chat_history\n",
        "    arguments[\"important_information\"] = important_information_prompt\n",
        "    arguments[\"user_input\"] = question\n",
        "\n",
        "    logging.info(\"Question: %s\", question)\n",
        "\n",
        "    answer = await kernel.invoke(\n",
        "        function_name=\"Chat\",\n",
        "        plugin_name=\"ChatBot\",\n",
        "        arguments=arguments,\n",
        "        chat_history=chat_history,\n",
        "    )\n",
        "\n",
        "    logging.info(\"Answer: %s\", answer)\n",
        "\n",
        "    # Log the question and answer to the chat history.\n",
        "    chat_history.add_user_message(question)\n",
        "    chat_history.add_message({\"role\": \"assistant\", \"message\": answer})\n",
        "\n",
        "    json_answer = json.loads(str(answer))\n",
        "\n",
        "    display(Markdown(json_answer[\"answer\"]))"
      ]
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "python310-sdkv2"
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      },
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
